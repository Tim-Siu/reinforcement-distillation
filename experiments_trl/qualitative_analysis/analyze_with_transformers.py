# analyze_with_transformers.py
import json
import argparse
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

# Use the same checkpoint mapping as in run_vllm_inference.py
MODEL_CHECKPOINTS = {}

INPUT_SAMPLES_FILE = "qualitative_analysis/data/selected_samples_for_analysis.jsonl"
INPUT_GENERATIONS_FILE = "qualitative_analysis/data/vllm_generations_redi-1-1_ft.jsonl" # Generated by run_vllm_inference.py
OUTPUT_ANALYSIS_FILE = "qualitative_analysis/data/analysis_results_redi-1-1_ft.jsonl"

tokenizer_cache = {}
chat_template_cache = {}

def get_tokenizer(model_path, trust_remote_code):
    if model_path not in tokenizer_cache:
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=trust_remote_code, padding_side='left')
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token # Common practice
            tokenizer_cache[model_path] = tokenizer
            
            # Attempt to get chat template string from tokenizer
            if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
                chat_template_cache[model_path] = tokenizer.chat_template
            elif hasattr(tokenizer, 'default_chat_template') and tokenizer.default_chat_template:
                chat_template_cache[model_path] = tokenizer.default_chat_template
            else: # Fallback if no template found on tokenizer
                print(f"Warning: No chat_template attribute found for tokenizer {model_path}. May affect formatting if not Qwen-like.")
                # For Qwen-like models, this might be okay if the apply_chat_template method works without it.
                # If you have custom jinja files and want to use them here too, you'd load them.
                # For this script, we primarily rely on tokenizer.apply_chat_template.
                chat_template_cache[model_path] = None # Signal that we rely on internal apply_chat_template logic
        except Exception as e:
            print(f"Error loading tokenizer for {model_path}: {e}")
            return None
    return tokenizer_cache[model_path]

def get_sequence_analytics(model, tokenizer, messages_list, device):
    results = {
        "normalized_logp": None,
        "avg_token_entropy": None,
        "token_logps": None,
        "token_mean_logits": None,
        "token_texts": None,
        "assistant_token_indices_in_full": None,
        "error": None
    }
    try:
        if not messages_list or messages_list[-1]['role'] != 'assistant' or not messages_list[-1]['content']:
            results["error"] = "Invalid messages_list: must end with non-empty assistant content."
            return results

        context_messages = messages_list[:-1]
        assistant_response_text = messages_list[-1]['content']

        templated_context_str = tokenizer.apply_chat_template(
            context_messages,
            add_generation_prompt=True, # Important for correct prompt formatting
            tokenize=False
        )
        # Tokenize context. Ensure add_special_tokens behavior is as expected.
        # If apply_chat_template already adds BOS for the model, add_special_tokens=False might be preferred.
        # However, for robustness, True is often used, assuming tokenizer handles duplicates or its absence.
        context_tokens = tokenizer(templated_context_str, return_tensors="pt", add_special_tokens=True)
        
        # Tokenize assistant's response text *without* special tokens or template wrappers.
        assistant_tokens = tokenizer(assistant_response_text, return_tensors="pt", add_special_tokens=False)

        context_input_ids = context_tokens.input_ids.to(device)
        assistant_input_ids = assistant_tokens.input_ids.to(device)

        if assistant_input_ids.shape[1] == 0:
            results["error"] = "Empty assistant response after tokenization."
            return results

        full_input_ids = torch.cat([context_input_ids, assistant_input_ids], dim=1)
        
        labels = full_input_ids.clone()
        labels[:, :context_input_ids.shape[1]] = -100 # Mask out context tokens

        with torch.no_grad():
            outputs = model(input_ids=full_input_ids, labels=labels) # attention_mask can often be inferred for unpadded single sequences
            logits = outputs.logits

        # Shift logits and labels for next token prediction
        # Logits for token i predict token i+1. Labels for token i are token i.
        # So, logits for [0...seq_len-2] predict labels [1...seq_len-1]
        shifted_logits = logits[..., :-1, :].contiguous()
        shifted_labels = labels[..., 1:].contiguous() # These are the target token IDs
        
        valid_labels_mask = (shifted_labels != -100) # Mask for actual assistant tokens
        if not valid_labels_mask.any():
            results["error"] = "No valid (non-masked) assistant tokens to analyze."
            return results

        actual_assistant_labels = shifted_labels[valid_labels_mask] # True token IDs of the assistant
        
        log_probs_full_vocab = torch.nn.functional.log_softmax(shifted_logits, dim=-1)
        
        # !!! THIS IS THE FIX !!!
        # Create a temporary version of shifted_labels for torch.gather
        # Replace -100 with a valid index (e.g., 0). These gathered values for originally -100 positions
        # will be ignored later by `valid_labels_mask`.
        shifted_labels_for_gather = shifted_labels.clone()
        shifted_labels_for_gather[shifted_labels == -100] = 0 # Or any valid token_id, e.g., tokenizer.pad_token_id if available and non-negative
        
        # Gather the logp of the target tokens using the modified labels
        token_logps_all_positions = torch.gather(log_probs_full_vocab, -1, shifted_labels_for_gather.unsqueeze(-1)).squeeze(-1)
        
        # Filter to get logps only for the assistant's tokens
        assistant_token_logps = token_logps_all_positions[valid_labels_mask]

        results["normalized_logp"] = assistant_token_logps.mean().item() if assistant_token_logps.numel() > 0 else 0.0
        results["token_logps"] = assistant_token_logps.cpu().tolist()
        
        # For entropy and mean logits, we need logits corresponding to assistant tokens
        # These are the logits that predicted the assistant's tokens
        assistant_logits = shifted_logits[valid_labels_mask] # Shape: [num_assistant_tokens, vocab_size]
        
        if assistant_logits.numel() > 0 and assistant_logits.shape[0] > 0: # Ensure non-empty tensor
            probs_dist = torch.softmax(assistant_logits, dim=-1)
            # Use log_softmax for numerical stability in entropy calculation
            log_probs_dist = torch.log_softmax(assistant_logits, dim=-1) 
            token_entropies = -torch.sum(probs_dist * log_probs_dist, dim=-1)
            results["avg_token_entropy"] = token_entropies.mean().item()
            results["token_mean_logits"] = assistant_logits.mean(dim=-1).cpu().tolist() # Mean over vocab dimension
        else:
            results["avg_token_entropy"] = 0.0
            results["token_mean_logits"] = []

        results["token_texts"] = tokenizer.convert_ids_to_tokens(actual_assistant_labels.cpu().numpy())
        
        # Calculate indices relative to the start of full_input_ids
        # The first token label corresponds to full_input_ids[1], its logit is from shifted_logits[0]
        # So assistant tokens start at context_input_ids.shape[1] in full_input_ids
        # The labels for these start at (context_input_ids.shape[1] - 1) in shifted_labels if non-empty context
        # Or more simply, find where valid_labels_mask is true.
        
        # The `actual_assistant_labels` are taken from `shifted_labels`.
        # `shifted_labels` corresponds to `full_input_ids[1:]`.
        # The first assistant token is `full_input_ids[context_input_ids.shape[1]]`.
        # Its label in `shifted_labels` is at index `context_input_ids.shape[1] - 1`.
        
        # Let's provide the indices of the assistant tokens within the `full_input_ids`
        start_idx_in_full_ids_for_assistant_tokens = context_input_ids.shape[1]
        end_idx_in_full_ids_for_assistant_tokens = full_input_ids.shape[1]
        results["assistant_token_indices_in_full"] = list(range(start_idx_in_full_ids_for_assistant_tokens, end_idx_in_full_ids_for_assistant_tokens))


    except Exception as e:
        results["error"] = str(e)
        # import traceback # Uncomment for debugging
        # traceback.print_exc() # Uncomment for debugging
        print(f"Error in get_sequence_analytics for input (first message role/content): {messages_list[0]['role']}/{messages_list[0]['content'][:50]}... Error: {e}")
    return results


def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    all_samples_for_analysis = []
    with open(INPUT_SAMPLES_FILE, "r") as f:
        for line in f:
            all_samples_for_analysis.append(json.loads(line))
    
    generations_for_analysis = []
    try:
        with open(INPUT_GENERATIONS_FILE, "r") as f:
            for line in f:
                generations_for_analysis.append(json.loads(line))
    except FileNotFoundError:
        print(f"Warning: {INPUT_GENERATIONS_FILE} not found. Only analyzing training samples from {INPUT_SAMPLES_FILE}.")

    analysis_output_file = open(OUTPUT_ANALYSIS_FILE, "w")

    for ckpt_name, model_path in MODEL_CHECKPOINTS.items():
        if model_path.startswith("/path/to/your"):
            print(f"Skipping placeholder checkpoint {ckpt_name}: {model_path}")
            continue

        print(f"\nLoading model and tokenizer for checkpoint: {ckpt_name} from {model_path}")
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_path, 
                torch_dtype=torch.bfloat16, 
                trust_remote_code=args.trust_remote_code,
                attn_implementation='flash_attention_2',
            ).to(device).eval()
            tokenizer = get_tokenizer(model_path, args.trust_remote_code)
            if tokenizer is None: 
                print(f"Could not load tokenizer for {model_path}, skipping checkpoint.")
                del model # cleanup
                if torch.cuda.is_available(): torch.cuda.empty_cache()
                continue
        except Exception as e:
            print(f"Error loading model {model_path}: {e}. Skipping checkpoint.")
            continue

        # 1. Analyze Training Samples
        print(f"  Analyzing training samples for {ckpt_name}...")
        for sample in tqdm(all_samples_for_analysis, desc=f"Train {ckpt_name[:10]}"):
            if sample["type"] != "train":
                continue
            
            if sample.get("chosen_full_conversation") and sample.get("chosen_assistant_text"):
                chosen_metrics = get_sequence_analytics(model, tokenizer, sample["chosen_full_conversation"], device)
                analysis_output_file.write(json.dumps({
                    "sample_id": sample["sample_id"], # Changed from sample_uuid
                    "original_item_identifier": sample["original_item_identifier"],
                    "checkpoint_name": ckpt_name,
                    "sample_type": "train_chosen",
                    "prompt_text": sample["prompt_text"],
                    "text_analyzed": sample["chosen_assistant_text"],
                    **chosen_metrics
                }) + "\n")

            if sample.get("rejected_full_conversation") and sample.get("rejected_assistant_text"):
                rejected_metrics = get_sequence_analytics(model, tokenizer, sample["rejected_full_conversation"], device)
                analysis_output_file.write(json.dumps({
                    "sample_id": sample["sample_id"], # Changed
                    "original_item_identifier": sample["original_item_identifier"],
                    "checkpoint_name": ckpt_name,
                    "sample_type": "train_rejected",
                    "prompt_text": sample["prompt_text"],
                    "text_analyzed": sample["rejected_assistant_text"],
                    **rejected_metrics
                }) + "\n")
        
        # 2. Analyze Test Generations
        print(f"  Analyzing test generations for {ckpt_name}...")
        for gen_sample in tqdm(generations_for_analysis, desc=f"Test Gen {ckpt_name[:10]}"):
            if gen_sample["checkpoint_name"] != ckpt_name:
                continue

            # Construct messages list for the generated sample
            # VLLM output `prompt_text_raw` is the original user prompt
            messages = [
                {"role": "user", "content": gen_sample["prompt_text_raw"]},
                {"role": "assistant", "content": gen_sample["generated_assistant_text"]}
            ]
            
            if not gen_sample["generated_assistant_text"]: # Skip if VLLM produced empty response
                print(f"Skipping empty generation for sample_id {gen_sample['sample_id']}, ckpt {ckpt_name}")
                continue

            generated_metrics = get_sequence_analytics(model, tokenizer, messages, device)
            analysis_output_file.write(json.dumps({
                "sample_id": gen_sample["sample_id"], # This is the hash ID of the original test prompt
                "checkpoint_name": ckpt_name,
                "chat_template_name": gen_sample.get("chat_template_name", "N/A"),
                "sample_type": "test_generated",
                "prompt_text": gen_sample["prompt_text_raw"], # Log the raw prompt
                "text_analyzed": gen_sample["generated_assistant_text"],
                **generated_metrics
            }) + "\n")
        
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    analysis_output_file.close()
    print(f"\nSaved analysis results to {OUTPUT_ANALYSIS_FILE}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze model outputs with Transformers.")
    parser.add_argument("--trust-remote-code", action="store_true", help="Trust remote code for model/tokenizer loading")
    args = parser.parse_args()
    main(args)
