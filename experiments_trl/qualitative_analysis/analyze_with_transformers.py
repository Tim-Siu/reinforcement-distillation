# analyze_with_transformers.py
import json
import argparse
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
from collections import defaultdict
MODEL_CHECKPOINTS = {}

INPUT_SAMPLES_FILE = "qualitative_analysis/data/selected_samples_for_analysis.jsonl"
INPUT_GENERATIONS_FILE = "qualitative_analysis/data/vllm_generations_redi-1-1_ft.jsonl" # Generated by run_vllm_inference.py
OUTPUT_ANALYSIS_FILE = "qualitative_analysis/data/analysis_results_redi-1-1_ft_full_with_predictions.jsonl"

tokenizer_cache = {}
chat_template_cache = {}

def get_tokenizer(model_path, trust_remote_code):
    if model_path not in tokenizer_cache:
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=trust_remote_code, padding_side='left')
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            tokenizer_cache[model_path] = tokenizer
            
            if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
                chat_template_cache[model_path] = tokenizer.chat_template
            elif hasattr(tokenizer, 'default_chat_template') and tokenizer.default_chat_template:
                chat_template_cache[model_path] = tokenizer.default_chat_template
            else:
                print(f"Warning: No chat_template attribute found for tokenizer {model_path}. May affect formatting if not Qwen-like.")
                chat_template_cache[model_path] = None
        except Exception as e:
            print(f"Error loading tokenizer for {model_path}: {e}")
            return None
    return tokenizer_cache[model_path]

def get_sequence_analytics(model, tokenizer, messages_list, device):
    results = {
        "normalized_logp": None,
        "avg_token_entropy": None,
        "token_logps": None,
        "token_mean_logits": None,
        "token_texts": None,
        "predicted_token_ids": None, ### ADDED ###
        "predicted_token_texts": None, ### ADDED ###
        "predicted_token_logps": None, ### ADDED ###
        "assistant_token_indices_in_full": None,
        "error": None
    }
    try:
        if not messages_list or messages_list[-1]['role'] != 'assistant' or not messages_list[-1]['content']:
            results["error"] = "Invalid messages_list: must end with non-empty assistant content."
            return results

        context_messages = messages_list[:-1]
        assistant_response_text = messages_list[-1]['content']

        templated_context_str = tokenizer.apply_chat_template(
            context_messages,
            add_generation_prompt=True,
            tokenize=False
        )
        context_tokens = tokenizer(templated_context_str, return_tensors="pt", add_special_tokens=True)
        assistant_tokens = tokenizer(assistant_response_text, return_tensors="pt", add_special_tokens=False)

        context_input_ids = context_tokens.input_ids.to(device)
        assistant_input_ids = assistant_tokens.input_ids.to(device)

        if assistant_input_ids.shape[1] == 0:
            results["error"] = "Empty assistant response after tokenization."
            return results

        full_input_ids = torch.cat([context_input_ids, assistant_input_ids], dim=1)
        labels = full_input_ids.clone()
        labels[:, :context_input_ids.shape[1]] = -100

        with torch.no_grad():
            outputs = model(input_ids=full_input_ids, labels=labels)
            logits = outputs.logits

        shifted_logits = logits[..., :-1, :].contiguous()
        shifted_labels = labels[..., 1:].contiguous()
        
        valid_labels_mask = (shifted_labels != -100)
        if not valid_labels_mask.any():
            results["error"] = "No valid (non-masked) assistant tokens to analyze."
            return results

        actual_assistant_labels = shifted_labels[valid_labels_mask]
        log_probs_full_vocab = torch.nn.functional.log_softmax(shifted_logits, dim=-1)
        
        shifted_labels_for_gather = shifted_labels.clone()
        shifted_labels_for_gather[shifted_labels == -100] = 0
        
        token_logps_all_positions = torch.gather(log_probs_full_vocab, -1, shifted_labels_for_gather.unsqueeze(-1)).squeeze(-1)
        assistant_token_logps = token_logps_all_positions[valid_labels_mask]

        results["normalized_logp"] = assistant_token_logps.mean().item() if assistant_token_logps.numel() > 0 else 0.0
        results["token_logps"] = assistant_token_logps.cpu().tolist()
        
        assistant_logits = shifted_logits[valid_labels_mask]
        
        if assistant_logits.numel() > 0 and assistant_logits.shape[0] > 0:
            probs_dist = torch.softmax(assistant_logits, dim=-1)
            log_probs_dist = torch.log_softmax(assistant_logits, dim=-1) 
            token_entropies = -torch.sum(probs_dist * log_probs_dist, dim=-1)
            results["avg_token_entropy"] = token_entropies.mean().item()
            results["token_mean_logits"] = assistant_logits.mean(dim=-1).cpu().tolist()

            # Get the log-probabilities of the tokens the model *would have* predicted (argmax)
            max_logprobs_predicted, predicted_token_ids_tensor = torch.max(log_probs_dist, dim=-1)
            
            results["predicted_token_ids"] = predicted_token_ids_tensor.cpu().tolist()
            results["predicted_token_texts"] = tokenizer.convert_ids_to_tokens(predicted_token_ids_tensor.cpu().numpy())
            results["predicted_token_logps"] = max_logprobs_predicted.cpu().tolist()
        else:
            results["avg_token_entropy"] = 0.0
            results["token_mean_logits"] = []
            results["predicted_token_ids"] = [] ### ADDED ###
            results["predicted_token_texts"] = [] ### ADDED ###
            results["predicted_token_logps"] = [] ### ADDED ###


        results["token_texts"] = tokenizer.convert_ids_to_tokens(actual_assistant_labels.cpu().numpy())
        
        start_idx_in_full_ids_for_assistant_tokens = context_input_ids.shape[1]
        end_idx_in_full_ids_for_assistant_tokens = full_input_ids.shape[1]
        results["assistant_token_indices_in_full"] = list(range(start_idx_in_full_ids_for_assistant_tokens, end_idx_in_full_ids_for_assistant_tokens))

    except Exception as e:
        results["error"] = str(e)
        print(f"Error in get_sequence_analytics for input (first message role/content): {messages_list[0]['role']}/{messages_list[0]['content'][:50]}... Error: {e}")
    return results

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    all_samples_for_analysis = []
    with open(INPUT_SAMPLES_FILE, "r") as f:
        for line in f:
            all_samples_for_analysis.append(json.loads(line))
    
    # Pre-sort generations by checkpoint for efficiency and clarity.
    generations_by_checkpoint = defaultdict(list)
    try:
        with open(INPUT_GENERATIONS_FILE, "r") as f:
            for line in f:
                generation = json.loads(line)
                # Group generations by the checkpoint that created them
                if "checkpoint_name" in generation:
                    generations_by_checkpoint[generation["checkpoint_name"]].append(generation)
    except FileNotFoundError:
        print(f"Warning: {INPUT_GENERATIONS_FILE} not found. Only analyzing training samples from {INPUT_SAMPLES_FILE}.")

    analysis_output_file = open(OUTPUT_ANALYSIS_FILE, "w")

    for ckpt_name, model_path in MODEL_CHECKPOINTS.items():
        if model_path.startswith("/path/to/your"):
            print(f"Skipping placeholder checkpoint {ckpt_name}: {model_path}")
            continue

        print(f"\nLoading model and tokenizer for checkpoint: {ckpt_name} from {model_path}")
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_path, 
                torch_dtype=torch.bfloat16, 
                trust_remote_code=args.trust_remote_code,
                attn_implementation='flash_attention_2' if torch.cuda.is_available() and hasattr(torch.nn.functional, 'scaled_dot_product_attention') else 'sdpa',
            ).to(device).eval()
            tokenizer = get_tokenizer(model_path, args.trust_remote_code)
            if tokenizer is None: 
                print(f"Could not load tokenizer for {model_path}, skipping checkpoint.")
                if 'model' in locals(): del model 
                if torch.cuda.is_available(): torch.cuda.empty_cache()
                continue
        except Exception as e:
            print(f"Error loading model {model_path}: {e}. Skipping checkpoint.")
            if 'model' in locals(): del model
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            continue

        # 1. Analyze Training Samples
        print(f"  Analyzing training samples for {ckpt_name}...")
        for sample in tqdm(all_samples_for_analysis, desc=f"Train {ckpt_name[:10]}"):
            if sample["type"] != "train":
                continue
            
            if sample.get("chosen_full_conversation") and sample.get("chosen_assistant_text"):
                chosen_metrics = get_sequence_analytics(model, tokenizer, sample["chosen_full_conversation"], device)
                analysis_output_file.write(json.dumps({
                    "sample_id": sample["sample_id"],
                    "original_item_identifier": sample["original_item_identifier"],
                    "checkpoint_name": ckpt_name,
                    "sample_type": "train_chosen",
                    "prompt_text": sample["prompt_text"],
                    "text_analyzed": sample["chosen_assistant_text"],
                    **chosen_metrics
                }) + "\n")

            if sample.get("rejected_full_conversation") and sample.get("rejected_assistant_text"):
                rejected_metrics = get_sequence_analytics(model, tokenizer, sample["rejected_full_conversation"], device)
                analysis_output_file.write(json.dumps({
                    "sample_id": sample["sample_id"],
                    "original_item_identifier": sample["original_item_identifier"],
                    "checkpoint_name": ckpt_name,
                    "sample_type": "train_rejected",
                    "prompt_text": sample["prompt_text"],
                    "text_analyzed": sample["rejected_assistant_text"],
                    **rejected_metrics
                }) + "\n")
        
        # Analyze test generations for this specific checkpoint
        print(f"  Analyzing test generations for {ckpt_name}...")
        
        # Retrieve only the generations for the currently loaded checkpoint
        generations_for_this_ckpt = generations_by_checkpoint.get(ckpt_name, [])

        if not generations_for_this_ckpt:
            print(f"    -> No generations found for '{ckpt_name}' in {INPUT_GENERATIONS_FILE}. Skipping test analysis.")
        else:
            for gen_sample in tqdm(generations_for_this_ckpt, desc=f"Test Gen {ckpt_name[:10]}"):
                # The check `if gen_sample["checkpoint_name"] != ckpt_name:` is no longer needed
                # as we are iterating over a pre-filtered list.
                
                messages = [
                    {"role": "user", "content": gen_sample["prompt_text_raw"]},
                    {"role": "assistant", "content": gen_sample["generated_assistant_text"]}
                ]
                
                if not gen_sample["generated_assistant_text"]:
                    print(f"Skipping empty generation for sample_id {gen_sample['sample_id']}, ckpt {ckpt_name}")
                    continue

                generated_metrics = get_sequence_analytics(model, tokenizer, messages, device)
                analysis_output_file.write(json.dumps({
                    "sample_id": gen_sample["sample_id"],
                    "checkpoint_name": ckpt_name,
                    "chat_template_name": gen_sample.get("chat_template_name", "N/A"),
                    "sample_type": "test_generated",
                    "prompt_text": gen_sample["prompt_text_raw"],
                    "text_analyzed": gen_sample["generated_assistant_text"],
                    **generated_metrics
                }) + "\n")
        
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    analysis_output_file.close()
    print(f"\nSaved analysis results to {OUTPUT_ANALYSIS_FILE}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze model outputs with Transformers.")
    parser.add_argument("--trust-remote-code", action="store_true", help="Trust remote code for model/tokenizer loading")
    args = parser.parse_args()
    main(args)
